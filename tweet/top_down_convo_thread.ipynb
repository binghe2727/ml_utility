{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# usage: given a tweet-id and retrieve the following tweet replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/binghe/Dropbox/gatech/github/aml/counter_misinformation'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet_look_up_by_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ad3bd02a3a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Replace your bearer token below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mBEAR_CDOE_BH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBEAR_CODE_SY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBEAR_CODE_GV\u001b[0m \u001b[0;31m# BEAR_CDOE_BH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwarc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbearer_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBEAR_CODE_SY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mREFERENCED_TWEET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/gatech/github/ml_utility/tweet/constant.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mbearer_code_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBEAR_CDOE_BH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBEAR_CODE_SY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBEAR_CODE_RS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBEAR_CODE_GV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBEAR_CODE_HG\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/gatech/github/ml_utility/tweet/constant.py\u001b[0m in \u001b[0;36mbearer_code_testing\u001b[0;34m(bearer_codes)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbearer_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbearer_codes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwarc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbearer_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbearer_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_look_up_by_id\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1402517058840039426\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweet_look_up_by_id' is not defined"
     ]
    }
   ],
   "source": [
    "from twarc import Twarc2, expansions\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "# Replace your bearer token below\n",
    "from constant import  BEAR_CDOE_BH, BEAR_CODE_SY, BEAR_CODE_GV # BEAR_CDOE_BH\n",
    "client = Twarc2(bearer_token=BEAR_CODE_SY)\n",
    "from constant import TEXT, ID, REFERENCED_TWEET\n",
    "from utility import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constant import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tweet_look_up_by_id(tweet_ids: list, client):\n",
    "    lookup = client.tweet_lookup(tweet_ids=tweet_ids)\n",
    "    tweet_res = []\n",
    "    try:\n",
    "        for page in lookup:\n",
    "            try:\n",
    "                result = expansions.flatten(page)\n",
    "                for tweet in result:\n",
    "                    tweet_res.append(tweet)\n",
    "            except ValueError:\n",
    "                print(f\"skip one unaccessible tweet id in one page and continue\")\n",
    "                continue\n",
    "        if len(tweet_res) < 1:\n",
    "            return None\n",
    "        else:\n",
    "            return tweet_res\n",
    "    except ValueError:\n",
    "        print(f\"encounter one unaccessible tweet id in one lookup(batch)\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def save_tweet_and_1st_reply(tweet, replies, csv_fp):\n",
    "    with open(csv_fp, \"w\", encoding=\"utf-8\") as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        header = [\"id\", \"tweet\", \"reply\", \"label\"]\n",
    "        csv_writer.writerow(header)\n",
    "        for index, reply in enumerate(replies):\n",
    "            if index == 0:\n",
    "                # change from tweet[ID] to reply[ID]\n",
    "                csv_writer.writerow([reply[ID], tweet[TEXT], reply[TEXT], \"\"])\n",
    "            else:\n",
    "                csv_writer.writerow([reply[ID], \"\", reply[TEXT], \"\"])\n",
    "        print(f\"we have {index} replies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utility import process_tweet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from utility import tweet_look_up_by_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if_filtering_short_tweets = False\n",
    "\n",
    "if if_filtering_short_tweets:\n",
    "    MIN_LEN_TWEET = 3 # after ieee bigdata'20 preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# condition 0: one csv with tweets in different timestamps\n",
    "# when I do the initial labeling of 500 tweets, I use this code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                               text  \\\n",
      "0  1342642741927432195  These rushed vaccines and their campaigns, wil...   \n",
      "1  1342622908468879368  Correct me if I’m wrong, but the COVID “vaccin...   \n",
      "2  1342585603825618944  Medical experts need to stop lying on behalf o...   \n",
      "3  1346724774203027456  https://t.co/3thOnspvkZ\\nn=2244 interviews wit...   \n",
      "4  1346557934684176386  Let's sum this up: The government demands that...   \n",
      "\n",
      "   #reply  \n",
      "0      11  \n",
      "1      12  \n",
      "2      10  \n",
      "3      10  \n",
      "4      14  \n"
     ]
    }
   ],
   "source": [
    "# from one example after classification\n",
    "# csv_fp = \"./toy_example_data/misinfo_only_tweet_2021-06-26.csv\"\n",
    "# original tweet annotation result\n",
    "# # replies: 15~50\n",
    "# csv_fp = \"./toy_example_data/anti_vax_misinfo_only_tweets_filtered_reply_15_to_50.csv\"\n",
    "# saved_convo_dir = \"./toy_example_data/anti_vax_convo_15_to_50\"\n",
    "# # of replies: 10~15\n",
    "csv_fp = \"./toy_example_data/anti_vax_misinfo_only_tweets_filtered_reply_10_to_15.csv\"\n",
    "saved_convo_dir = \"./toy_example_data/anti_vax_convo_10_to_15\"\n",
    "\n",
    "df = pd.read_csv(csv_fp)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ===== a list of tweet ids for conversation setup ====\n",
    "tweets = tweet_look_up_by_id(df[\"id\"].tolist(), client)\n",
    "# ==== only 1 tweet conversation ====\n",
    "# tweets = tweet_look_up_by_id( [1402517058840039426], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " process tweet-id: 1342642741927432195, convo-id: 1342642741927432195\n",
      "we have 9 replies\n",
      " process tweet-id: 1342622908468879368, convo-id: 1342622908468879368\n",
      "we have 9 replies\n",
      " process tweet-id: 1342585603825618944, convo-id: 1342585603825618944\n",
      "we have 2 replies\n",
      " process tweet-id: 1346724774203027456, convo-id: 1346724774203027456\n",
      "we have 4 replies\n",
      " process tweet-id: 1346557934684176386, convo-id: 1346557934684176386\n",
      "we have 9 replies\n",
      " process tweet-id: 1346344148874498049, convo-id: 1346344148874498049\n",
      "we have 9 replies\n",
      " process tweet-id: 1346323182094057472, convo-id: 1346323182094057472\n",
      "we have 9 replies\n",
      " process tweet-id: 1346107112749989888, convo-id: 1346107112749989888\n",
      "we have 3 replies\n",
      " process tweet-id: 1346045122719322114, convo-id: 1346045122719322114\n",
      "we have 4 replies\n",
      " process tweet-id: 1345757507319443457, convo-id: 1345757507319443457\n",
      "we have 7 replies\n",
      " process tweet-id: 1344999203894865922, convo-id: 1344999203894865922\n",
      "we have 10 replies\n",
      " process tweet-id: 1360772730077859843, convo-id: 1360772730077859843\n",
      "we have 4 replies\n",
      " process tweet-id: 1360625157543391234, convo-id: 1360625157543391234\n",
      "we have 10 replies\n",
      " process tweet-id: 1360015299844661251, convo-id: 1360015299844661251\n",
      "we have 7 replies\n",
      " process tweet-id: 1359103049751486473, convo-id: 1359103049751486473\n",
      "we have 9 replies\n",
      " process tweet-id: 1374054992319946753, convo-id: 1374054992319946753\n",
      "we have 8 replies\n",
      " process tweet-id: 1372962781142585350, convo-id: 1372962781142585350\n",
      "we have 5 replies\n",
      " process tweet-id: 1372466943299620866, convo-id: 1372466943299620866\n",
      "we have 3 replies\n",
      " process tweet-id: 1372331481142595584, convo-id: 1372331481142595584\n",
      "we have 9 replies\n",
      " process tweet-id: 1372175883880964103, convo-id: 1372175883880964103\n",
      "we have 6 replies\n",
      " process tweet-id: 1371492641922240515, convo-id: 1371492641922240515\n",
      "we have 5 replies\n",
      " process tweet-id: 1387769051049611264, convo-id: 1387769051049611264\n",
      "we have 2 replies\n",
      " process tweet-id: 1387179331827507201, convo-id: 1387179331827507201\n",
      "we have 5 replies\n",
      " process tweet-id: 1387104489706901504, convo-id: 1387104489706901504\n",
      "we have 3 replies\n",
      " process tweet-id: 1387069054951870464, convo-id: 1387069054951870464\n",
      "we have 7 replies\n",
      " process tweet-id: 1386334999692124161, convo-id: 1386334999692124161\n",
      "we have 11 replies\n",
      " process tweet-id: 1386199822877696001, convo-id: 1386199822877696001\n",
      "we have 5 replies\n",
      " process tweet-id: 1385581362225328130, convo-id: 1385581362225328130\n",
      "we have 9 replies\n",
      " process tweet-id: 1389637429527728135, convo-id: 1389637429527728135\n",
      "we have 8 replies\n",
      " process tweet-id: 1389327342741377036, convo-id: 1389327342741377036\n",
      "we have 5 replies\n",
      " process tweet-id: 1389221672608227330, convo-id: 1389221672608227330\n",
      "we have 6 replies\n",
      " process tweet-id: 1389196756424998922, convo-id: 1389196756424998922\n",
      "we have 6 replies\n",
      " process tweet-id: 1389065868793974784, convo-id: 1389065868793974784\n",
      "we have 5 replies\n",
      " process tweet-id: 1389007452423680002, convo-id: 1389007452423680002\n",
      "we have 10 replies\n",
      " process tweet-id: 1388297924786991104, convo-id: 1388297924786991104\n",
      "we have 5 replies\n"
     ]
    }
   ],
   "source": [
    "# look up the whole conversation from a tweet-id\n",
    "for tweet_obj in tweets:\n",
    "    tweet_id = tweet_obj[ID]\n",
    "\n",
    "    convo_csv_fp = f\"{saved_convo_dir}/{tweet_id}.csv\"\n",
    "    #%%\n",
    "    created_at = tweet_obj[CREATED_AT]\n",
    "    #%%\n",
    "    # print(created_at)\n",
    "    #%%\n",
    "    year = created_at[0:4]\n",
    "    month = created_at[5:7]\n",
    "    day = created_at[8:10]\n",
    "    #%%\n",
    "    conv_id = tweet_obj[CONVERSATION_ID]\n",
    "    print(f\" process tweet-id: {tweet_id}, convo-id: {conv_id}\")\n",
    "    start_time = datetime.datetime(int(year), int(month), int(day), 0, 0, 0, 0,\n",
    "                                   datetime.timezone.utc)\n",
    "    #%%\n",
    "    query = f\"conversation_id:{conv_id}\"\n",
    "    search_results = client.search_all(query=query,\n",
    "                                       start_time=start_time, max_results=100)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    referenced_id2tweet_obj = {}\n",
    "    for page in search_results:\n",
    "        # put the page loop first, since, we may not have the conversations\n",
    "        result = expansions.flatten(page)\n",
    "        for tweet in result:\n",
    "            # print(json.dumps(tweet))\n",
    "            # output_file.write(f\"{json.dumps(tweet)}\\n\")\n",
    "            # print(tweet)\n",
    "            # print(tweet[ID])\n",
    "            # ==== check the length of the tweet ====\n",
    "            if if_filtering_short_tweets and len(word_tokenize(process_tweet(tweet[TEXT]))) < MIN_LEN_TWEET:\n",
    "                continue\n",
    "            # ==== end ====\n",
    "            referenced_tweet_id = tweet[REFERENCED_TWEET][0][ID]\n",
    "            if referenced_tweet_id in referenced_id2tweet_obj:\n",
    "                referenced_id2tweet_obj[referenced_tweet_id].append(tweet)\n",
    "            else:\n",
    "                referenced_id2tweet_obj[referenced_tweet_id] = [tweet]\n",
    "\n",
    "    save_tweet_and_1st_reply(tweet_obj,\n",
    "                             referenced_id2tweet_obj[tweet_obj[ID]],\n",
    "                             convo_csv_fp)\n",
    "    # print(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# condition 1: other cases: we have csvs in a directory and\n",
    "# save the convo in year-month-day/id.csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# year = 2021\n",
    "# month = 6\n",
    "# day = 26\n",
    "\n",
    "csv_dir = \"/home/bhe46/data/ANTiVax/tweet_mis_only_csv\"\n",
    "saved_convo_dir_parent = \"/home/bhe46/data/ANTiVax/tweet_mis_only_csv_then_convo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process file: /home/bhe46/data/ANTiVax/tweet_mis_only_csv/2021-05-07.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-95640eba4555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"#reply\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# ==== end ====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_look_up_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "for csv_fn in os.listdir(csv_dir):\n",
    "    csv_fp = f\"{csv_dir}/{csv_fn}\"\n",
    "    print(f\"process file: {csv_fp}\")\n",
    "    year = csv_fp[-14:-10]\n",
    "    month = csv_fp[-9:-7]\n",
    "    day = csv_fp[-6:-4]\n",
    "\n",
    "    convo_csv_dir = f\"{saved_convo_dir_parent}/{year}-{month}-{day}\"\n",
    "\n",
    "    if not os.path.exists(convo_csv_dir):\n",
    "        os.makedirs(convo_csv_dir)\n",
    "\n",
    "\n",
    "    df = pd.read_csv(csv_fp)\n",
    "    # ==== only get tweets with more than 3 replies ====\n",
    "    df = df[ df[\"#reply\"] >= 3 ]\n",
    "    # ==== end ====\n",
    "    tweets = tweet_look_up_by_id(df[\"id\"].tolist(), client)\n",
    "\n",
    "\n",
    "    for tweet_obj in tweets:\n",
    "        tweet_id = tweet_obj[ID]\n",
    "\n",
    "        convo_csv_fp = f\"{convo_csv_dir}/{tweet_id}.csv\"\n",
    "        #%%\n",
    "        created_at = tweet_obj[CREATED_AT]\n",
    "        #%%\n",
    "        # print(created_at)\n",
    "        #%%\n",
    "        year = created_at[0:4]\n",
    "        month = created_at[5:7]\n",
    "        day = created_at[8:10]\n",
    "        #%%\n",
    "        conv_id = tweet_obj[CONVERSATION_ID]\n",
    "        print(f\" process tweet-id: {tweet_id}, convo-id: {conv_id}\")\n",
    "        start_time = datetime.datetime(int(year), int(month), int(day), 0, 0, 0, 0,\n",
    "                                       datetime.timezone.utc)\n",
    "        #%%\n",
    "        query = f\"conversation_id:{conv_id}\"\n",
    "        search_results = client.search_all(query=query,\n",
    "                                           start_time=start_time, max_results=100)\n",
    "\n",
    "        #%%\n",
    "\n",
    "        referenced_id2tweet_obj = {}\n",
    "        for page in search_results:\n",
    "            # put the page loop first, since, we may not have the conversations\n",
    "            result = expansions.flatten(page)\n",
    "            for tweet in result:\n",
    "                # print(json.dumps(tweet))\n",
    "                # output_file.write(f\"{json.dumps(tweet)}\\n\")\n",
    "                # print(tweet)\n",
    "                # print(tweet[ID])\n",
    "                # ==== check the length of the tweet ====\n",
    "                if if_filtering_short_tweets and len(word_tokenize(process_tweet(tweet[TEXT]))) < MIN_LEN_TWEET:\n",
    "                    continue\n",
    "                # ==== end ====\n",
    "                referenced_tweet_id = tweet[REFERENCED_TWEET][0][ID]\n",
    "                if referenced_tweet_id in referenced_id2tweet_obj:\n",
    "                    referenced_id2tweet_obj[referenced_tweet_id].append(tweet)\n",
    "                else:\n",
    "                    referenced_id2tweet_obj[referenced_tweet_id] = [tweet]\n",
    "\n",
    "        save_tweet_and_1st_reply(tweet_obj,\n",
    "                                 referenced_id2tweet_obj[tweet_obj[ID]],\n",
    "                                 convo_csv_fp)\n",
    "        # print(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# condition 2: tweet_id == convo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(csv_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "saved_convo_dir = f\"./toy_example_data/{year}_{month}_{day}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for tweet_id in list(df[ID]):\n",
    "    tweet_obj = tweet_look_up_by_id([tweet_id], client)[0]\n",
    "    tweet_id = tweet_obj[ID]\n",
    "    convo_csv_fp = f\"{saved_convo_dir}/{tweet_id}.csv\"\n",
    "\n",
    "\n",
    "    conv_id = tweet_obj[CONVERSATION_ID]\n",
    "    query = f\"conversation_id:{conv_id}\"\n",
    "    search_results = client.search_all(query=query,\n",
    "                                       start_time=start_time, max_results=100)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    referenced_id2tweet_obj = {}\n",
    "    for page in search_results:\n",
    "        # put the page loop first, since, we may not have the conversations\n",
    "        result = expansions.flatten(page)\n",
    "        for tweet in result:\n",
    "            # print(json.dumps(tweet))\n",
    "            # output_file.write(f\"{json.dumps(tweet)}\\n\")\n",
    "            # print(tweet)\n",
    "            # print(tweet[ID])\n",
    "            referenced_tweet_id = tweet[REFERENCED_TWEET][0][ID]\n",
    "            if referenced_tweet_id in referenced_id2tweet_obj:\n",
    "                referenced_id2tweet_obj[referenced_tweet_id].append(tweet)\n",
    "            else:\n",
    "                referenced_id2tweet_obj[referenced_tweet_id] = [tweet]\n",
    "\n",
    "    save_tweet_and_1st_reply(tweet_obj,\n",
    "                             referenced_id2tweet_obj[tweet_obj[ID]],\n",
    "                             convo_csv_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# condition 2: tweet_id != convo_id, then, we only save the related first-level setup\n",
    "# then, the setup is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 2: save the file, one convo, one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we process it the classification step\n",
    "# df = pd.read_csv(\"./toy_example_data/top_down_test.csv\")\n",
    "# df[\"processed_reply\"] = df[\"reply\"].apply(tokenize)\n",
    "# df.to_csv(\"./toy_example_data/top_down_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f59bae883a455fedcaa2c21324b622883ba3e9f6b88b455d79a918a5d986103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
